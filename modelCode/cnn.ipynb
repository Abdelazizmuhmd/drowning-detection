{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projectcode (3).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3D4Q6owXATO",
        "outputId": "22898ee2-1dbe-465c-8bfc-2d48fcc55c43"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHvoEizsa-Gz"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import joblib\n",
        "# load the binarized labels file\n",
        "lb = joblib.load('/content/gdrive/MyDrive/all/outputs/lb.pkl')\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
        "        self.conv4 = nn.Conv2d(64, 128, 5)\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.fc2 = nn.Linear(256, len(lb.classes_))\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        bs, _, _, _ = x.shape\n",
        "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlIaGoi92E-d"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import argparse\n",
        "import joblib\n",
        "import cv2\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import albumentations\n",
        "from torchvision.transforms import transforms   \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNAW9V4t2JyM",
        "outputId": "4e67720e-49d6-4ce2-a8cb-694c39ed016e"
      },
      "source": [
        "print('Loading model and label binarizer...')\n",
        "lb = joblib.load('/content/gdrive/MyDrive/all/outputs/lb.pkl')\n",
        "model = CustomCNN().cuda()\n",
        "print('Model Loaded...')\n",
        "model.load_state_dict(torch.load('/content/gdrive/MyDrive/all/outputs/mymodeldone3.pth'))\n",
        "print('Loaded model state_dict...')\n",
        "aug = albumentations.Compose([\n",
        "    albumentations.Resize(224, 224),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model and label binarizer...\n",
            "Model Loaded...\n",
            "Loaded model state_dict...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rDT3jn-3skr",
        "outputId": "e5e6e772-db75-479e-e2e7-53d798c477cc"
      },
      "source": [
        "%cd gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqVxkSqh3GYb",
        "outputId": "02bed191-1e01-47ae-deab-c57f86345a10"
      },
      "source": [
        "# read until end of video\n",
        "import time\n",
        "class detection:\n",
        "  drowning = []\n",
        "\n",
        "  def detectDrowning(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    countDrowning=0\n",
        "    countNormal=0\n",
        "    if (cap.isOpened() == False):\n",
        "        print('Error while trying to read video. Plese check again...')\n",
        "    # get the frame width and height\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    # define codec and create VideoWriter object\n",
        "    out = cv2.VideoWriter(str('/content/projecttest2.mp4'), cv2.VideoWriter_fourcc(*'mp4v'), 30, (1080,1920))\n",
        "    #print(fps)\n",
        "    while(cap.isOpened()):\n",
        "        # capture each frame of the video\n",
        "        start_time = time.time() # start time of the loop\n",
        "        ret, frame = cap.read()\n",
        "        if ret == True:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                # conver to PIL RGB format before predictions\n",
        "                pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "                pil_image = aug(image=np.array(pil_image))['image']\n",
        "                pil_image = np.transpose(pil_image, (2, 0, 1)).astype(np.float32)\n",
        "                pil_image = torch.tensor(pil_image, dtype=torch.float).cuda()\n",
        "                pil_image = pil_image.unsqueeze(0)\n",
        "                outputs = model(pil_image)\n",
        "                _, preds = torch.max(outputs.data, 1)\n",
        "            if lb.classes_[preds]=='drowning':\n",
        "              self.drowning.append(\"1\")\n",
        "              if len(drowning) == 10\"\n",
        "                alertDrowning()\n",
        "                classification = 'drowning'\n",
        "            else:\n",
        "                self.drowning.clear()\n",
        "                classification = 'normal'\n",
        "\n",
        "\n",
        "            #classification = lb.classes_[preds]\n",
        "            activiy = \"activiy: \" +  classification\n",
        "            \n",
        "            frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
        "            cv2.putText(frame,activiy, (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 200, 0), 3)\n",
        "            cv2_imshow(frame)\n",
        "            out.write(frame)\n",
        "            # press `q` to exit\n",
        "            \n",
        "            if cv2.waitKey(27) & 0xFF == ord('q'):\n",
        "                print(\"classified as normal:\",countNormal)\n",
        "                print(\"classified as drowning: \",countDrowning)\n",
        "                break\n",
        "        else: \n",
        "            print(\"classified as normal:\",countNormal)\n",
        "            print(\"classified as drowning: \",countDrowning)\n",
        "            break   \n",
        "            \n",
        "        #print(\"FPS: \", 1.0 / (time.time() - start_time)) \n",
        "    if countDrowning >=  countNormal:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "    # release VideoCapture()\n",
        "    cap.release()\n",
        "    # close all frames and video windows\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "  def alertDrowning():\n",
        "    organizationId = \"Ahlyclub49301616522931404\"\n",
        "    sent = False\n",
        "    text = \"drowning alert\"\n",
        "    db = firestore.client()\n",
        "    collectionBane = db.collection(\"lifeguardnotifications\").add({\n",
        "      \"orgID\": organizationId,\n",
        "      \"sent\": sent,\n",
        "      \"text\": text\n",
        "    })\n",
        "video_path = '/content/gdrive/MyDrive/Dataset/Normal/normal_left/IMG_0351.MOV'\n",
        "d = detection()\n",
        "d.detectDrowning(video_path)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classified as normal: 171\n",
            "classified as drowning:  0\n",
            "Alert as not drowning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_we0hoY16skY"
      },
      "source": [
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "cred = credentials.Certificate(\"pooleye-68d49-firebase-adminsdk-zkgaj-cfc002cbda.json\")\n",
        "firebase_admin.initialize_app(cred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZN5qkNY6sYJ"
      },
      "source": [
        ""
      ]
    }
  ]
}